"""
Train TransE baseline model fro KB Completion

USAGE

python3 train.py --data data --name NEWS --batch_size 128 --num_epochs 50 --validation_step 10 \
--debug_nTrain 100 --debug_nVal 100

"""
import json
import torch
import torch.optim as optim
from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau
from torch.nn import MarginRankingLoss

from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
from builddata import *
from batching import BatchLoader
from logger import Logger
from transE import TransE
from DataLoader import OpenKELoader

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Parameters
# ==================================================
parser = ArgumentParser("TransE", formatter_class=ArgumentDefaultsHelpFormatter, conflict_handler='resolve')

parser.add_argument("--data", default="data/NEWS", help="Data sources.", required=True)
parser.add_argument("--name", default="NEWS", required=True, help="Model name. Will store model files in directory with name ")

parser.add_argument("--batch_size", default=128, type=int, help="Batch Size")
parser.add_argument("--neg_ratio", default=1.0, type=float, help="Number of negative triples generated by positive")
parser.add_argument("--margin", default=1.0, type=float, help="Margin")
parser.add_argument("--norm", default=2, type=float, help="L-norm (1 or 2) used in TransE scoring function")
parser.add_argument("--patience", default=10, type=int, help="Stop training after no improvement in validation loss for this many epochs")
parser.add_argument("--learning_rate", default=0.001, type=float, help="Learning rate")
parser.add_argument("--num_epochs", default=200, type=int, help="Number of training epochs")
parser.add_argument("--embedding_dim", default=100, type=int, help="Dimensionality of character embedding")
parser.add_argument("--validation_step", default=1, type=int, help="Number of epochs to train between validations")
parser.add_argument("--init_pretrained", default=False, help="Load in pretrained embeddings from tok2vec.init", action="store_true")
parser.add_argument("--l2reg", default=0, type=float, help="L-2 regularization term")
parser.add_argument("--optim", default='adam', type=str, help="Optimizer adam or sgd")

# MODEL PARAMS
parser.add_argument("--use_source", default=False, help="Flag to use source information", action="store_true")

# DEV ARGS
parser.add_argument("--debug_nTrain", default=-1, type=int, help="Num training samples to use")
parser.add_argument("--debug_nVal", default=-1, type=int, help="Num validation samples to use")
parser.add_argument("--debug_nTest", default=-1, type=int, help="Num test samples to use")

args = parser.parse_args()
print(args)


model_dir = args.name


# LOAD DATA
# =========================================
print("Loading data...")

train, val, test, bernoulli_p, goldens, \
all_ents, all_rels, rel2id, ent2id, src2id  = build_data(path=args.data)
all_sources = list(src2id.values()) #[0,1]

print("\nLoaded {} entities".format(len(all_ents)))
print("Loaded {} relations\n".format(len(all_rels)))

# DEV - Subset training and validation sets
if args.debug_nTrain > 0:
	print("subsetting data to {} Train".format(min(len(train), args.debug_nTrain)))
	train = train[:min(len(train), args.debug_nTrain)]
	

if args.debug_nVal > 0:
	print("subsetting data to {} Val".format(min(args.debug_nVal, len(val))))
	val = val[:min(args.debug_nVal, len(val))]

if args.debug_nTest > 0:
	print("subsetting data to {} Test".format(min(args.debug_nTest, len(test))))
	test = test[:min(args.debug_nTest, len(test))]

# LOAD CONFIG
# =========================================
with open(os.path.join(model_dir, 'config.json'), 'r') as f:
	config = json.load(f)

# BATCH LOADER
# =========================================
loader = BatchLoader(train, bernoulli_p, goldens, all_ents, all_sources, batch_size=args.batch_size, neg_ratio=args.neg_ratio)

print("Loading data... finished!")

# INITIALIZE MODEL
# =========================================
transE = TransE(len(all_rels), len(ent2id), len(src2id), 
				dim=args.embedding_dim, norm=args.norm, 
				use_source_info=args.use_source)
transE.to(device)

criterion = MarginRankingLoss(args.margin, False).to(device)
if args.optim== 'adam':
	optimizer = optim.Adam(transE.parameters(), lr=args.learning_rate, weight_decay=args.l2reg)
else:
	optimizer = optim.SGD(transE.parameters(), lr=args.learning_rate, weight_decay=args.l2reg)


# learning rate updates
scheduler=ReduceLROnPlateau(optimizer, factor=0.1, 
	patience=3, threshold=1e-2, verbose=True)

# Geneate corrupt samples for validation - faster to do this once 
# val_dict = loader.validation_triples(val)

print("====================================\n")



print("\nTESTING")
print("====================================\n")


# EVAL ON TEST SET
transE.load_state_dict(torch.load(os.path.join(model_dir, 'best_val_loss_state_dict.pt')))
transE.eval()


test_dict = loader.validation_triples(test)
MR, MRR, h10 = transE.validate(test, test_dict, all_ents, all_rels, all_sources)
print("\n\nRESULTS ON TEST SET")
print("====================================")
print("MR = {}, MRR = {:.4}, H@10 = {:.4}\n".format(round(MR), MRR, h10))


with open(os.path.join(model_dir, 'test_eval.tsv'), 'w') as f:
	f.write("MR\tMRR\tH@10\n")
	f.write("{}\t{}\t{}\n".format(MR, MRR, h10))











