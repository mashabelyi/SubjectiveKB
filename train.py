"""
Train TransE baseline model fro KB Completion

USAGE

python3 train.py --data data --name NEWS --batch_size 128 --num_epochs 50 --validation_step 10 \
--debug_nTrain 100 --debug_nVal 100

"""
import json
import torch
import torch.optim as optim
from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau
from torch.nn import MarginRankingLoss

from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
from builddata import *
from batching import BatchLoader
from logger import Logger
from transE import TransE
from DataLoader import OpenKELoader

import pickle

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Parameters
# ==================================================
parser = ArgumentParser("TransE", formatter_class=ArgumentDefaultsHelpFormatter, conflict_handler='resolve')

parser.add_argument("--data", default="data/NEWS", help="Data sources.", required=True)
parser.add_argument("--name", default="NEWS", required=True, help="Model name. Will store model files in directory with name ")

parser.add_argument("--test_pkl", required=False, help="Path to pickle file with test dict")
parser.add_argument("--val_pkl", required=False, help="Path to pickle file with val dict")

parser.add_argument("--batch_size", default=128, type=int, help="Batch Size")
parser.add_argument("--neg_ratio", default=1.0, type=float, help="Number of negative triples generated by positive")
parser.add_argument("--margin", default=1.0, type=float, help="Margin")
parser.add_argument("--norm", default=2, type=float, help="L-norm (1 or 2) used in TransE scoring function")
parser.add_argument("--patience", default=10, type=int, help="Stop training after no improvement in validation loss for this many epochs")
parser.add_argument("--learning_rate", default=0.001, type=float, help="Learning rate")
parser.add_argument("--num_epochs", default=200, type=int, help="Number of training epochs")
parser.add_argument("--embedding_dim", default=100, type=int, help="Dimensionality of character embedding")
parser.add_argument("--validation_step", default=1, type=int, help="Number of epochs to train between validations")
parser.add_argument("--init_pretrained", default=False, help="Load in pretrained embeddings from tok2vec.init", action="store_true")
parser.add_argument("--l2reg", default=0, type=float, help="L-2 regularization term")
parser.add_argument("--optim", default='adam', type=str, help="Optimizer adam or sgd")

# MODEL PARAMS
parser.add_argument("--use_source", default=False, help="Flag to use source information", action="store_true")

# DEV ARGS
parser.add_argument("--debug_nTrain", default=-1, type=int, help="Num training samples to use")
parser.add_argument("--debug_nVal", default=-1, type=int, help="Num validation samples to use")
parser.add_argument("--debug_nTest", default=-1, type=int, help="Num test samples to use")

args = parser.parse_args()
print(args)

# =========================================
#
# MODEL FOLDER
#
# =========================================

if os.path.exists(args.name):
	overwrite = input("\noverwrite existing model folder '{}'? type y/n: ".format(args.name))
	if overwrite!='y':
		print("choose a different model name (--name)")
		exit(0)
if not os.path.exists(args.name):
	os.makedirs(args.name)
model_dir = args.name

logger = Logger(model_dir, ['loss', 'val_loss', 'MR', 'MRR', 'h@10'])

# =========================================
# LOAD DATA
# =========================================

print("Loading data...")

if os.path.isfile(os.path.join(args.data, 'train2id.txt')):
	train, val, test, bernoulli_p, goldens, \
	all_ents, all_rels, rel2id, ent2id, src2id  = OpenKELoader(path=args.data)
	all_sources = []
else:
	train, val, test, bernoulli_p, goldens, \
	all_ents, all_rels, rel2id, ent2id, src2id  = build_data(path=args.data)
	all_sources = list(src2id.values()) #[0,1]

print("\nLoaded {} entities".format(len(all_ents)))
print("Loaded {} relations\n".format(len(all_rels)))

# =========================================
# DEV - Subset training and validation sets
# =========================================
if args.debug_nTrain > 0:
	print("subsetting data to {} Train".format(min(len(train), args.debug_nTrain)))
	train = train[:min(len(train), args.debug_nTrain)]
	

if args.debug_nVal > 0:
	print("subsetting data to {} Val".format(min(args.debug_nVal, len(val))))
	val = val[:min(args.debug_nVal, len(val))]

if args.debug_nTest > 0:
	print("subsetting data to {} Test".format(min(args.debug_nTest, len(test))))
	test = test[:min(args.debug_nTest, len(test))]


# =========================================
# SAVE CONFIG
# save input arguments in model folder
# =========================================

config = args.__dict__
config['numTrain'] = len(train)
config['numVal'] = len(val)
config['numTest'] = len(test)
config['numEntities'] = len(all_ents)
config['numRelations'] = len(all_rels)
with open(os.path.join(model_dir, 'config.json'), 'w') as f:
	json.dump(config, f, indent=2)


# =========================================
# BATCH LOADER
# =========================================
loader = BatchLoader(train, bernoulli_p, goldens, all_ents, all_sources, batch_size=args.batch_size, neg_ratio=args.neg_ratio)

print("Loading data... finished!")

# =========================================
# INITIALIZE MODEL
# =========================================
transE = TransE(len(all_rels), len(ent2id), dim=args.embedding_dim, norm=args.norm)
transE.to(device)


# =========================================
# OPTIMIZER + LOSS FUNCTION
# =========================================
criterion = MarginRankingLoss(args.margin, False).to(device)
if args.optim== 'adam':
	optimizer = optim.Adam(transE.parameters(), lr=args.learning_rate, weight_decay=args.l2reg)
else:
	optimizer = optim.SGD(transE.parameters(), lr=args.learning_rate, weight_decay=args.l2reg)


# =========================================
# LEARNING RATE UPDATES
# =========================================
scheduler=ReduceLROnPlateau(optimizer, factor=0.1, 
	patience=3, threshold=1e-2, verbose=True)

# Geneate corrupt samples for validation - faster to do this once 
# val_dict = loader.validation_triples(val)

print("====================================\n")

# Log validation metrics before training
# MR, MRR, h10 = transE.validate(val, val_dict, all_ents, all_rels, all_sources)
# MR, MRR, h10 = transE.fast_validate(val)
# print("      MR = {}, MR = {:.4}, H@10 = {:.4}".format(round(MR), MRR, h10))
# logger.log(-1, {'MR':MR, 'MRR':MRR, 'h@10':h10})



# =========================================
#
# TRAIN
#
# =========================================


minValLoss = float('inf')
maxMRR = 0
maxH10 = 0


patience = args.patience
for epoch in range(args.num_epochs):
	transE.train()

	print("epoch {}".format(epoch+1), end='\r')
	epochLoss = 0
	loader.reset()
	b = 0
	while loader.has_next():
		b += 1
		print("epoch {} batch {}".format(epoch, b), end='\r')
		
		pos, neg = loader.next_batch()

		posScore, negScore = transE(pos[:,0], pos[:,1], pos[:,2], pos[:,3], 
									neg[:,0], neg[:,1], neg[:,2], neg[:,3])

		tmpTensor = torch.tensor([-1], dtype=torch.float).to(device)
		batchLoss = criterion(posScore, negScore, tmpTensor)
		batchLoss.backward()
		optimizer.step()
		epochLoss += batchLoss

	
	transE.eval()

	# VALIDATION LOSS
	valLoss = 0
	pos_batches, neg_batches = loader.validaton_batches(val)
	for pos, neg in zip(pos_batches, neg_batches):
		# run through network
		posScore, negScore = transE(pos[:,0], pos[:,1], pos[:,2], pos[:,3], 
									neg[:,0], neg[:,1], neg[:,2], neg[:,3])

		tmpTensor = torch.tensor([-1], dtype=torch.float).to(device)
		valLoss += criterion(posScore, negScore, tmpTensor)


	# Report loss per sample
	epochLoss = epochLoss.item()/config['numTrain']
	valLoss = valLoss.item()/config['numVal']
	print("epoch {} - loss: {:.8}, val_loss: {:.8}".format(epoch, epochLoss, valLoss))
	

	# scheduler.step(epochLoss) # this seems to prevent the network from training well 

	if epoch%args.validation_step == 0 and epoch>1:  # or epoch == 0
		# VALIDATE
		MR, MRR, h10 = transE.validate(val, val_dict, all_ents, all_rels, all_sources)
		print("      MR = {}, MRR = {:.4}, H@10 = {:.4}".format(round(MR), MRR, h10))

		logger.log(epoch, {'loss':epochLoss, 'val_loss':valLoss, 'MR':MR, 'MRR':MRR, 'h@10':h10})

		# SAVE MODEL WEIGHTS
		# =========================================
		if MRR > maxMRR:
			maxMRR = MRR
			torch.save(transE.state_dict(), os.path.join(model_dir, 'best_MRR_state_dict.pt'))

		if h10 > maxH10:
			maxH10 = h10
			torch.save(transE.state_dict(), os.path.join(model_dir, 'best_H10_state_dict.pt'))

	else:
		logger.log(epoch, {'loss':epochLoss, 'val_loss':valLoss})

	
	# SAVE MODEL WEIGHTS
	# =========================================
	if valLoss < minValLoss:
		torch.save(transE.state_dict(), os.path.join(model_dir, 'best_val_loss_state_dict.pt'))
		minValLoss = valLoss
		patience = args.patience
	else:
		patience -= 1

		if patience == 0:
			print("Stopping early")
			break

print("\nTRAINING FINISHED")
print("====================================\n")

print("Best Val Loss = {}".format(minValLoss))
print("Best Val MRR = {}".format(maxMRR))
print("Best Val H@10 = {}".format(maxH10))

# EVAL ON VAL SET
transE.load_state_dict(torch.load(os.path.join(model_dir, 'best_val_loss_state_dict.pt')))
transE.eval()


if args.val_pkl:
	print("Loading val from pickle file")
	val_dict = pickle.load(open(args.val_pkl,"rb"))
else:
	val_dict = loader.validation_triples(val)

MR, MRR, h10 = transE.validate(val, val_dict, all_ents, all_rels, all_sources)
print("\n\nRESULTS ON VAL SET")
print("====================================")
print("MR = {}, MRR = {:.4}, H@10 = {:.4}\n".format(round(MR), MRR, h10))


with open(os.path.join(model_dir, 'val_eval.tsv'), 'w') as f:
	f.write("MR\tMRR\tH@10\n")
	f.write("{}\t{}\t{}\n".format(MR, MRR, h10))


# if args.test_pkl:
# 	print("Loading test from pickle file")
# 	test_dict = pickle.load(open(args.test_pkl,"rb"))
# else:
# 	test_dict = loader.validation_triples(test)

# MR, MRR, h10 = transE.validate(test, test_dict, all_ents, all_rels, all_sources)
# print("\n\nRESULTS ON TEST SET")
# print("====================================")
# print("MR = {}, MRR = {:.4}, H@10 = {:.4}\n".format(round(MR), MRR, h10))


# with open(os.path.join(model_dir, 'test_eval.tsv'), 'w') as f:
# 	f.write("MR\tMRR\tH@10\n")
# 	f.write("{}\t{}\t{}\n".format(MR, MRR, h10))











